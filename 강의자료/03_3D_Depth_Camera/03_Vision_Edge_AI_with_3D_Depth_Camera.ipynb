{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Edge-AI와 3D Depth Camera\n",
    "\n",
    "3D Depth Camera는 공간의 깊이 정보를 캡처하여 3차원 데이터로 변환하는 장치입니다. 이러한 카메라는 다양한 응용 분야에서 사용되며, 컴퓨터 비전, 로봇 공학, 증강현실(AR), 가상현실(VR), 의료 이미지 처리 등에서 중요합니다. 본 강의에서는 3D Depth Camera의 원리, 기술, 응용 사례, 그리고 DeepStream을 활용할 수 있는 방법에 대해 설명합니다.\n",
    "\n",
    "## 03-2. 3D Depth Camera\n",
    "\n",
    "### 3D Depth Camera의 정의\n",
    "3D Depth Camera는 객체나 장면의 깊이 정보를 캡처할 수 있는 장치로, 카메라가 각 픽셀의 거리 값을 캡처하여 3차원 공간에서 물체를 인식하거나 측정할 수 있도록 합니다.\n",
    "\n",
    "### 3D Depth Camera의 원리\n",
    "1. **스테레오 비전 (Stereo Vision)**:\n",
    "    * 두 개의 카메라로 물체를 촬영한 이미지를 비교하여 깊이를 계산합니다.\n",
    "    * 사람의 시각 시스템과 유사한 방식입니다.\n",
    "\n",
    "2. **구조광 (Structured Light)**:\n",
    "    * 특정 패턴의 빛을 물체에 투사하고, 왜곡된 패턴을 분석하여 깊이를 측정합니다.\n",
    "    * 대표적인 예: Microsoft Kinect\n",
    "\n",
    "3. **비행시간 (Time of Flight, ToF)**:\n",
    "    * 빛이 물체에 반사되어 돌아오는 시간을 측정하여 깊이를 계산합니다.\n",
    "    * 정확하고 빠른 깊이 측정이 가능합니다.\n",
    "\n",
    "4. **LiDAR (Light Detection and Ranging)**:\n",
    "    * 레이저를 사용해 주변 환경을 스캔하여 깊이를 측정합니다.\n",
    "    * 자율주행차에 주로 사용됩니다.\n",
    "\n",
    "### 응용 분야\n",
    "1. **산업 및 제조**:\n",
    "    * 로봇 비전: 로봇의 정확한 동작과 위치 결정을 지원.\n",
    "    * 품질 검사: 제품의 결함 및 크기 측정.\n",
    "2. **의료**:\n",
    "    * 3D 스캔: 인체를 스캔하여 맞춤형 의료 장비 제작.\n",
    "    * 수술 보조: 정확한 수술 계획을 위한 3D 모델 생성.\n",
    "3. **엔터테인먼트**:\n",
    "    * 증강현실/가상현실(AR/VR): 몰입형 환경 구축.\n",
    "    * 게임: 사용자 동작을 추적하여 인터랙션 제공.\n",
    "4. **도매(Retail)**:\n",
    "    * 고객 행동 분석: 매장 내 고객의 동선을 추적하고 행동을 분석.\n",
    "    * 재고 관리: 제품 크기 및 위치를 자동으로 스캔하여 재고를 효율적으로 관리.\n",
    "    * 무인 매장: 3D 데이터를 활용해 고객과 제품의 상호작용을 자동화."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03-3. OAK-D 모듈\n",
    "- **OAK-D 모듈 소개**: OAK-D(OpenCV AI Kit Depth)는 AI 및 컴퓨터 비전 기능을 갖춘 카메라 모듈로, 실시간으로 깊이 정보를 제공하며 다양한 응용 분야에 적합합니다.\n",
    "\n",
    "- **주요 특징**:\n",
    "  1. **내장형 AI**: Myriad X VPU를 탑재하여 온보드 AI 처리 가능.\n",
    "  2. **스테레오 비전**: 두 개의 모노 카메라로 깊이 정보 계산.\n",
    "  3. **컬러 카메라**: 고해상도 컬러 카메라로 세부 정보를 캡처.\n",
    "  4. **플러그 앤 플레이**: USB를 통해 쉽게 연결 및 사용 가능.\n",
    "  5. **다양한 SDK 지원**: DepthAI 및 OpenCV와 같은 SDK 지원.\n",
    "\n",
    "- **응용 사례**:\n",
    "  - 로봇 내비게이션\n",
    "  - 객체 추적 및 감지\n",
    "  - 증강 현실 및 가상 현실 애플리케이션\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 간단한 예제 실습\n",
    "\n",
    "## Jetson Orin Nano에서 OAK-D PRO setting (Connection)\n",
    "\n",
    "### 준비물\n",
    "- Jetson Orin Nano\n",
    "- OAK-D PRO Camera\n",
    "- USB-C 케이블\n",
    "\n",
    "### 필수 라이브러리 설치\n",
    "라이브러리 설치\n",
    "```bash\n",
    "# 설치되어있는 OpenCV 삭제\n",
    "$ pip uninstall opencv-python\n",
    "\n",
    "# 저장소 추가\n",
    "$ sudo add-apt-repository universe\n",
    "$ sudo apt update\n",
    "\n",
    "# 필수 라이브러리 설치\n",
    "$ sudo apt install -y build-essential cmake git pkg-config libgtk-3-dev libavcodec-dev libavformat-dev libswscale-dev libv4l-dev libxvidcore-dev libx264-dev libjpeg-dev libpng-dev libtiff-dev gfortran openexr libatlas-base-dev python3-dev python3-numpy libtbb2 libtbb-dev libdc1394-dev\n",
    "\n",
    "# OpenCV 설치\n",
    "pip install opencv-python\n",
    "```\n",
    "\n",
    "OAK-D 관련 드라이버 설치\n",
    "```bash\n",
    "$ wget -qO- https://docs.luxonis.com/install_dependencies.sh | bash\n",
    "$ pip install depthai --upgrade\n",
    "```\n",
    "* 드라이버 설치 이후 반드시 OAK-D 모듈 연결 USB를 뺐다가 다시 꽂기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to connect to device, error message: X_LINK_DEVICE_ALREADY_IN_USE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m cam\u001b[38;5;241m.\u001b[39mvideo\u001b[38;5;241m.\u001b[39mlink(xout\u001b[38;5;241m.\u001b[39minput)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Start the pipeline\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mdai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m device:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline started\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     video \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mgetOutputQueue(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to connect to device, error message: X_LINK_DEVICE_ALREADY_IN_USE"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import depthai as dai\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Create ColorCamera node\n",
    "cam = pipeline.create(dai.node.ColorCamera)\n",
    "cam.setBoardSocket(dai.CameraBoardSocket.CAM_A)  # CAM_A로 수정\n",
    "cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "cam.setVideoSize(1920, 1080)\n",
    "\n",
    "# Create XLinkOut node\n",
    "xout = pipeline.create(dai.node.XLinkOut)\n",
    "xout.setStreamName(\"video\")\n",
    "\n",
    "# Connect camera to output\n",
    "cam.video.link(xout.input)\n",
    "\n",
    "# Start the pipeline\n",
    "with dai.Device(pipeline) as device:\n",
    "    print(\"Pipeline started\")\n",
    "    video = device.getOutputQueue(name=\"video\", maxSize=4, blocking=False)\n",
    "    \n",
    "    while True:\n",
    "        # Get frame\n",
    "        frame = video.get().getCvFrame()\n",
    "        \n",
    "        # Show frame\n",
    "        cv2.imshow(\"OAK-D Video Feed\", frame)\n",
    "        \n",
    "        # Exit on 'q' key\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OAK-D와 DeepStream을 결합한 ObjectDetection\n",
    "\n",
    "### 준비물\n",
    "* Jetson\n",
    "* DeepStream 설치\n",
    "* OAK-D 카메라 모듈\n",
    "\n",
    "### 실행 준비\n",
    "라이브러리 설치\n",
    "```bash\n",
    "sudo apt update\n",
    "\n",
    "# C++ 표준 라이브러리\n",
    "sudo apt install build-essential g++ libstdc++-12-dev\n",
    "\n",
    "# nlohmann-json 라이브러리 설치\n",
    "sudo apt install nlohmann-json3-dev\n",
    "```\n",
    "\n",
    "OAK-D를 DeepStream 연결을 위한 DepthAI C++ 라이브러리 플러그인 설치\n",
    "```bash\n",
    "$ git clone --recurse-submodules https://github.com/luxonis/depthai-core.git\n",
    "# 서브모듈 초기화 및 업데이트\n",
    "$ git submodule update --init --recursive\n",
    "$ cd depthai-core\n",
    "$ mkdir build\n",
    "$ cd build\n",
    "$ cmake ..\n",
    "$ make -j$(nproc)\n",
    "\n",
    "echo \"export OPENBLAS_CORETYPE=ARMV8\" >> ~/.bashrc\n",
    "\n",
    "# nlohmann json 다운로드\n",
    "$ cd ../shared/depthai-shared/include\n",
    "$ mkdir nlohmann\n",
    "$ cd nlohmann\n",
    "$ wget https://github.com/nlohmann/json/releases/download/v3.11.2/json.hpp\n",
    "\n",
    "#tl/optional.hpp 다운로드\n",
    "$ cd ..\n",
    "$ mkdir tl\n",
    "$ cd tl\n",
    "$ wget https://raw.githubusercontent.com/TartanLlama/optional/master/include/tl/optional.hpp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting min object dimensions as 16x16 instead of 1x1 to support VIC compute mode.\n",
      "WARNING: [TRT]: Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "Implicit layer support has been deprecated\n",
      "INFO: [Implicit Engine Info]: layers num: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:04:42.876802696 \u001b[35m 4546\u001b[00m 0xaaaad8ea23c0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:684:gst_nvinfer_logger:<nvinfer2>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::deserializeEngineAndBackend() <nvdsinfer_context_impl.cpp:2092> [UID = 1]: deserialized trt engine from :/home/paymentinapp/Desktop/deepstream/DeepStream/강의자료/03_3D_Depth_Camera/resnet18_trafficcamnet_pruned.onnx_b1_gpu0_int8.engine\n",
      "0:04:42.876911919 \u001b[35m 4546\u001b[00m 0xaaaad8ea23c0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:684:gst_nvinfer_logger:<nvinfer2>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::generateBackendContext() <nvdsinfer_context_impl.cpp:2195> [UID = 1]: Use deserialized engine model: /home/paymentinapp/Desktop/deepstream/DeepStream/강의자료/03_3D_Depth_Camera/resnet18_trafficcamnet_pruned.onnx_b1_gpu0_int8.engine\n",
      "0:04:43.019696089 \u001b[35m 4546\u001b[00m 0xaaaad8ea23c0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer_impl.cpp:343:notifyLoadModelStatus:<nvinfer2>\u001b[00m [UID 1]: Load new model:./dstest1_pgie_config.txt sucessfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-20 13:15:33.205] [depthai] [warning] Skipping X_LINK_UNBOOTED device with name \"1.2.3\" ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dvs/git/dirty/git-master_linux/nvutils/nvbufsurftransform/nvbufsurftransform.cpp:4814: => NvVicCompose Failed\n",
      "\n",
      "/dvs/git/dirty/git-master_linux/nvutils/nvbufsurftransform/nvbufsurftransform.cpp:4819: => RGB/BGR Format transformation is not supported by VIC use GPU instead\n",
      "\n",
      "0:04:47.859624660 \u001b[35m 4546\u001b[00m 0xffff38000da0 \u001b[31;01mERROR  \u001b[00m \u001b[00m      nvvideoconvert gstnvvideoconvert.c:4255:gst_nvvideoconvert_transform:\u001b[00m buffer transform failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline stopped and cleaned up.\n"
     ]
    }
   ],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import Gst\n",
    "\n",
    "# GStreamer 초기화\n",
    "Gst.init(None)\n",
    "\n",
    "# GStreamer 파이프라인 생성 함수\n",
    "def create_gstreamer_pipeline():\n",
    "    pipeline_str = \"\"\"\n",
    "        appsrc name=source ! videoconvert ! video/x-raw,format=BGR ! \\\n",
    "        nvvideoconvert gpu-id=0 ! video/x-raw(memory:NVMM) ! \\\n",
    "        nvinfer config-file-path=./dstest1_pgie_config.txt ! \\\n",
    "        nvdsosd ! video/x-raw(memory:NVMM),format=RGBA ! \\\n",
    "        nvvidconv ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink name=sink\n",
    "    \"\"\"\n",
    "    return Gst.parse_launch(pipeline_str)\n",
    "\n",
    "# OAK-D 파이프라인 생성 함수\n",
    "def create_oak_d_pipeline():\n",
    "    pipeline = dai.Pipeline()\n",
    "    cam = pipeline.createColorCamera()\n",
    "    cam.setPreviewSize(1920, 1080)  # 스트림 너비와 높이를 1920x1080으로 설정\n",
    "    cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "    cam.setInterleaved(False)\n",
    "    cam.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "    xout = pipeline.createXLinkOut()\n",
    "    xout.setStreamName(\"video\")\n",
    "    cam.preview.link(xout.input)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# GStreamer에서 처리된 프레임 가져오기\n",
    "def get_frame_from_gstreamer(sink):\n",
    "    sample = sink.emit(\"pull-sample\")\n",
    "    if not sample:\n",
    "        return None\n",
    "\n",
    "    buffer = sample.get_buffer()\n",
    "    caps = sample.get_caps()\n",
    "    width = caps.get_structure(0).get_value(\"width\")\n",
    "    height = caps.get_structure(0).get_value(\"height\")\n",
    "\n",
    "    # 버퍼 데이터를 NumPy 배열로 변환\n",
    "    success, map_info = buffer.map(Gst.MapFlags.READ)\n",
    "    if not success:\n",
    "        return None\n",
    "\n",
    "    frame = np.ndarray(\n",
    "        (height, width, 3), dtype=np.uint8, buffer=map_info.data\n",
    "    )\n",
    "    buffer.unmap(map_info)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# GStreamer로 프레임 전달 함수\n",
    "def push_frame_to_gstreamer(source, frame):\n",
    "    # OpenCV 이미지를 GStreamer에 전달 가능한 포맷으로 변환\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # GStreamer 버퍼 생성\n",
    "    buf = Gst.Buffer.new_allocate(None, frame.nbytes, None)\n",
    "    buf.fill(0, frame.tobytes())\n",
    "    buf.pts = Gst.util_uint64_scale(Gst.CLOCK_TIME_NONE, 1, Gst.SECOND)\n",
    "\n",
    "    # Caps 설정\n",
    "    caps = Gst.Caps.from_string(\n",
    "        f\"video/x-raw,format=BGR,width={width},height={height},framerate=30/1\"\n",
    "    )\n",
    "    source.set_property(\"caps\", caps)\n",
    "\n",
    "    # GStreamer로 버퍼 전달\n",
    "    source.emit(\"push-buffer\", buf)\n",
    "\n",
    "# 메인 실행\n",
    "def main():\n",
    "    gst_pipeline = None  # gst_pipeline 초기화\n",
    "    try:\n",
    "        # GStreamer 파이프라인 생성 및 실행\n",
    "        gst_pipeline = create_gstreamer_pipeline()\n",
    "        source = gst_pipeline.get_by_name(\"source\")\n",
    "        sink = gst_pipeline.get_by_name(\"sink\")\n",
    "        gst_pipeline.set_state(Gst.State.PLAYING)\n",
    "\n",
    "        # OAK-D 파이프라인 생성\n",
    "        oak_d_pipeline = create_oak_d_pipeline()\n",
    "\n",
    "        # OAK-D 장치 초기화 및 데이터 스트림 처리\n",
    "        with dai.Device(oak_d_pipeline) as device:\n",
    "            queue = device.getOutputQueue(name=\"video\", maxSize=4, blocking=False)\n",
    "\n",
    "            while True:\n",
    "                in_frame = queue.get()\n",
    "                frame = in_frame.getCvFrame()\n",
    "\n",
    "                # OpenCV로 OAK-D 원본 프레임 확인 (디버깅용)\n",
    "                cv2.imshow(\"OAK-D Frame\", frame)\n",
    "\n",
    "                # GStreamer로 프레임 전달\n",
    "                push_frame_to_gstreamer(source, frame)\n",
    "\n",
    "                # GStreamer 처리 결과를 가져와 OpenCV로 디스플레이\n",
    "                gstreamer_frame = get_frame_from_gstreamer(sink)\n",
    "                if gstreamer_frame is not None:\n",
    "                    cv2.imshow(\"Object Detection\", gstreamer_frame)\n",
    "\n",
    "                if cv2.waitKey(1) == ord('q'):\n",
    "                    break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # gst_pipeline이 생성된 경우에만 상태를 NULL로 설정\n",
    "        if gst_pipeline:\n",
    "            gst_pipeline.set_state(Gst.State.NULL)\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Pipeline stopped and cleaned up.\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting min object dimensions as 16x16 instead of 1x1 to support VIC compute mode.\n",
      "WARNING: [TRT]: Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.\n",
      "Implicit layer support has been deprecated\n",
      "INFO: [Implicit Engine Info]: layers num: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:10:59.908246985 \u001b[35m 4546\u001b[00m 0xaaaad8ea23c0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:684:gst_nvinfer_logger:<nvinfer4>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::deserializeEngineAndBackend() <nvdsinfer_context_impl.cpp:2092> [UID = 1]: deserialized trt engine from :/home/paymentinapp/Desktop/deepstream/DeepStream/강의자료/03_3D_Depth_Camera/resnet18_trafficcamnet_pruned.onnx_b1_gpu0_int8.engine\n",
      "0:10:59.908478930 \u001b[35m 4546\u001b[00m 0xaaaad8ea23c0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:684:gst_nvinfer_logger:<nvinfer4>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::generateBackendContext() <nvdsinfer_context_impl.cpp:2195> [UID = 1]: Use deserialized engine model: /home/paymentinapp/Desktop/deepstream/DeepStream/강의자료/03_3D_Depth_Camera/resnet18_trafficcamnet_pruned.onnx_b1_gpu0_int8.engine\n",
      "0:10:59.911835445 \u001b[35m 4546\u001b[00m 0xaaaad8ea23c0 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer_impl.cpp:343:notifyLoadModelStatus:<nvinfer4>\u001b[00m [UID 1]: Load new model:./dstest1_pgie_config.txt sucessfully\n",
      "/dvs/git/dirty/git-master_linux/nvutils/nvbufsurftransform/nvbufsurftransform.cpp:4814: => NvVicCompose Failed\n",
      "\n",
      "/dvs/git/dirty/git-master_linux/nvutils/nvbufsurftransform/nvbufsurftransform.cpp:4819: => RGB/BGR Format transformation is not supported by VIC use GPU instead\n",
      "\n",
      "0:11:02.867113822 \u001b[35m 4546\u001b[00m 0xffff38000da0 \u001b[31;01mERROR  \u001b[00m \u001b[00m      nvvideoconvert gstnvvideoconvert.c:4255:gst_nvvideoconvert_transform:\u001b[00m buffer transform failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvstreammux: Successfully handled EOS for source_id=0\n",
      "Pipeline stopped and cleaned up.\n"
     ]
    }
   ],
   "source": [
    "import depthai as dai\n",
    "import cv2\n",
    "import numpy as np\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import Gst\n",
    "\n",
    "# GStreamer 초기화\n",
    "Gst.init(None)\n",
    "\n",
    "# GStreamer 파이프라인 생성 함수\n",
    "def create_gstreamer_pipeline():\n",
    "    pipeline_str = \"\"\"\n",
    "        appsrc name=source ! videoconvert ! video/x-raw,format=BGR,width=1920,height=1080 ! \\\n",
    "        nvvideoconvert gpu-id=0 ! video/x-raw(memory:NVMM),format=NV12 ! \\\n",
    "        muxer.sink_0 nvstreammux name=muxer width=1920 height=1080 batch-size=1 batched-push-timeout=40000 ! \\\n",
    "        nvinfer config-file-path=./dstest1_pgie_config.txt ! \\\n",
    "        nvdsosd gpu-id=0 ! \\\n",
    "        nvvidconv gpu-id=0 ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink name=sink\n",
    "    \"\"\"\n",
    "    return Gst.parse_launch(pipeline_str)\n",
    "\n",
    "# OAK-D 파이프라인 생성 함수\n",
    "def create_oak_d_pipeline():\n",
    "    pipeline = dai.Pipeline()\n",
    "    cam = pipeline.createColorCamera()\n",
    "    cam.setPreviewSize(1920, 1080)  # 스트림 너비와 높이를 1920x1080으로 설정\n",
    "    cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "    cam.setInterleaved(False)\n",
    "    cam.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "    xout = pipeline.createXLinkOut()\n",
    "    xout.setStreamName(\"video\")\n",
    "    cam.preview.link(xout.input)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# GStreamer에서 처리된 프레임 가져오기\n",
    "def get_frame_from_gstreamer(sink):\n",
    "    sample = sink.emit(\"pull-sample\")\n",
    "    if not sample:\n",
    "        return None\n",
    "\n",
    "    buffer = sample.get_buffer()\n",
    "    caps = sample.get_caps()\n",
    "    width = caps.get_structure(0).get_value(\"width\")\n",
    "    height = caps.get_structure(0).get_value(\"height\")\n",
    "\n",
    "    # 버퍼 데이터를 NumPy 배열로 변환\n",
    "    success, map_info = buffer.map(Gst.MapFlags.READ)\n",
    "    if not success:\n",
    "        return None\n",
    "\n",
    "    frame = np.ndarray(\n",
    "        (height, width, 3), dtype=np.uint8, buffer=map_info.data\n",
    "    )\n",
    "    buffer.unmap(map_info)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# GStreamer로 프레임 전달 함수\n",
    "def push_frame_to_gstreamer(source, frame):\n",
    "    # OpenCV 이미지를 GStreamer에 전달 가능한 포맷으로 변환\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # GStreamer 버퍼 생성\n",
    "    buf = Gst.Buffer.new_allocate(None, frame.nbytes, None)\n",
    "    buf.fill(0, frame.tobytes())\n",
    "    buf.pts = Gst.util_uint64_scale(Gst.CLOCK_TIME_NONE, 1, Gst.SECOND)\n",
    "\n",
    "    # Caps 설정\n",
    "    caps = Gst.Caps.from_string(\n",
    "        f\"video/x-raw,format=BGR,width={width},height={height},framerate=30/1\"\n",
    "    )\n",
    "    source.set_property(\"caps\", caps)\n",
    "\n",
    "    # GStreamer로 버퍼 전달\n",
    "    source.emit(\"push-buffer\", buf)\n",
    "\n",
    "# 메인 실행\n",
    "def main():\n",
    "    gst_pipeline = None  # gst_pipeline 초기화\n",
    "    try:\n",
    "        # GStreamer 파이프라인 생성 및 실행\n",
    "        gst_pipeline = create_gstreamer_pipeline()\n",
    "        source = gst_pipeline.get_by_name(\"source\")\n",
    "        sink = gst_pipeline.get_by_name(\"sink\")\n",
    "        gst_pipeline.set_state(Gst.State.PLAYING)\n",
    "\n",
    "        # OAK-D 파이프라인 생성\n",
    "        oak_d_pipeline = create_oak_d_pipeline()\n",
    "\n",
    "        # OAK-D 장치 초기화 및 데이터 스트림 처리\n",
    "        with dai.Device(oak_d_pipeline) as device:\n",
    "            queue = device.getOutputQueue(name=\"video\", maxSize=4, blocking=False)\n",
    "\n",
    "            while True:\n",
    "                in_frame = queue.get()\n",
    "                frame = in_frame.getCvFrame()\n",
    "\n",
    "                # OpenCV로 OAK-D 원본 프레임 확인 (디버깅용)\n",
    "                cv2.imshow(\"OAK-D Frame\", frame)\n",
    "\n",
    "                # GStreamer로 프레임 전달\n",
    "                push_frame_to_gstreamer(source, frame)\n",
    "\n",
    "                # GStreamer 처리 결과를 가져와 OpenCV로 디스플레이\n",
    "                gstreamer_frame = get_frame_from_gstreamer(sink)\n",
    "                if gstreamer_frame is not None:\n",
    "                    cv2.imshow(\"Object Detection\", gstreamer_frame)\n",
    "\n",
    "                if cv2.waitKey(1) == ord('q'):\n",
    "                    break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # gst_pipeline이 생성된 경우에만 상태를 NULL로 설정\n",
    "        if gst_pipeline:\n",
    "            gst_pipeline.set_state(Gst.State.NULL)\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Pipeline stopped and cleaned up.\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: gst_parse_error: could not link nvvideoconvert11 to muxer, muxer can't handle caps video/x-raw(memory:NVMM) (3)\n",
      "Pipeline stopped.\n"
     ]
    }
   ],
   "source": [
    "import depthai as dai\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import Gst\n",
    "\n",
    "# GStreamer 초기화\n",
    "Gst.init(None)\n",
    "\n",
    "# GStreamer 파이프라인 생성 함수\n",
    "def create_gstreamer_pipeline():\n",
    "    pipeline_str = \"\"\"\n",
    "        appsrc name=source ! videoconvert ! video/x-raw,format=BGR ! \\\n",
    "        nvvideoconvert gpu-id=0 ! video/x-raw(memory:NVMM) ! \\\n",
    "        nvstreammux name=muxer width=1920 height=1080 batch-size=1 batched-push-timeout=40000 ! \\\n",
    "        nvinfer config-file-path=./dstest1_pgie_config.txt ! \\\n",
    "        nvdsosd gpu-id=0 ! \\\n",
    "        nvvidconv gpu-id=0 ! video/x-raw,format=BGRx ! videoconvert ! video/x-raw,format=BGR ! appsink name=sink\n",
    "    \"\"\"\n",
    "    return Gst.parse_launch(pipeline_str)\n",
    "\n",
    "# OAK-D 파이프라인 생성 함수\n",
    "def create_oak_d_pipeline():\n",
    "    pipeline = dai.Pipeline()\n",
    "    cam = pipeline.createColorCamera()\n",
    "    cam.setPreviewSize(1920, 1080)  # 해상도 동기화\n",
    "    cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "    cam.setInterleaved(False)\n",
    "    cam.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "    xout = pipeline.createXLinkOut()\n",
    "    xout.setStreamName(\"video\")\n",
    "    cam.preview.link(xout.input)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# 메타데이터 디버깅 함수\n",
    "def process_metadata(sample):\n",
    "    \"\"\"GStreamer 추론 결과 메타데이터 디버깅\"\"\"\n",
    "    buf = sample.get_buffer()\n",
    "    print(\"Metadata buffer received:\", buf)\n",
    "\n",
    "# GStreamer에서 처리된 프레임 가져오기\n",
    "def get_frame_from_gstreamer(sink):\n",
    "    sample = sink.emit(\"pull-sample\")\n",
    "    if not sample:\n",
    "        return None\n",
    "\n",
    "    process_metadata(sample)\n",
    "    return True\n",
    "\n",
    "# GStreamer로 프레임 전달 함수\n",
    "def push_frame_to_gstreamer(source, frame):\n",
    "    height, width, _ = frame.shape\n",
    "    buf = Gst.Buffer.new_allocate(None, frame.nbytes, None)\n",
    "    buf.fill(0, frame.tobytes())\n",
    "    buf.pts = Gst.util_uint64_scale(Gst.CLOCK_TIME_NONE, 1, Gst.SECOND)\n",
    "\n",
    "    caps = Gst.Caps.from_string(f\"video/x-raw,format=BGR,width={width},height={height},framerate=30/1\")\n",
    "    source.set_property(\"caps\", caps)\n",
    "    source.emit(\"push-buffer\", buf)\n",
    "\n",
    "# 메인 실행\n",
    "def main():\n",
    "    gst_pipeline = None\n",
    "    try:\n",
    "        gst_pipeline = create_gstreamer_pipeline()\n",
    "        source = gst_pipeline.get_by_name(\"source\")\n",
    "        sink = gst_pipeline.get_by_name(\"sink\")\n",
    "        gst_pipeline.set_state(Gst.State.PLAYING)\n",
    "\n",
    "        oak_d_pipeline = create_oak_d_pipeline()\n",
    "        with dai.Device(oak_d_pipeline) as device:\n",
    "            queue = device.getOutputQueue(name=\"video\", maxSize=4, blocking=False)\n",
    "\n",
    "            while True:\n",
    "                in_frame = queue.get()\n",
    "                frame = in_frame.getCvFrame()\n",
    "\n",
    "                push_frame_to_gstreamer(source, frame)\n",
    "                if get_frame_from_gstreamer(sink):\n",
    "                    print(\"Inference completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    finally:\n",
    "        if gst_pipeline:\n",
    "            gst_pipeline.set_state(Gst.State.NULL)\n",
    "        print(\"Pipeline stopped.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GStreamer pipeline...\n",
      "Error: gst_parse_error: could not link nvvideoconvert13 to muxer, muxer can't handle caps video/x-raw(memory:NVMM), format=(string)NV12, width=(int)1920, height=(int)1080 (3)\n",
      "Pipeline stopped.\n"
     ]
    }
   ],
   "source": [
    "import depthai as dai\n",
    "import gi\n",
    "import numpy as np\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import Gst\n",
    "\n",
    "# GStreamer 초기화\n",
    "Gst.init(None)\n",
    "\n",
    "# GStreamer 파이프라인 생성 함수\n",
    "def create_gstreamer_pipeline():\n",
    "    pipeline_str = \"\"\"\n",
    "        appsrc name=source ! videoconvert ! video/x-raw,format=BGR,width=1920,height=1080 ! \\\n",
    "        nvvideoconvert gpu-id=0 ! video/x-raw(memory:NVMM),format=NV12,width=1920,height=1080 ! \\\n",
    "        nvstreammux name=muxer batch-size=1 width=1920 height=1080 batched-push-timeout=40000 ! \\\n",
    "        nvinfer config-file-path=./dstest1_pgie_config.txt ! \\\n",
    "        nvdsosd gpu-id=0 ! \\\n",
    "        nvvidconv ! video/x-raw,format=BGR ! videoconvert ! autovideosink\n",
    "    \"\"\"\n",
    "    return Gst.parse_launch(pipeline_str)\n",
    "\n",
    "# OAK-D 파이프라인 생성 함수\n",
    "def create_oak_d_pipeline():\n",
    "    pipeline = dai.Pipeline()\n",
    "    cam = pipeline.createColorCamera()\n",
    "    cam.setPreviewSize(1920, 1080)  # 해상도를 1920x1080으로 설정\n",
    "    cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "    cam.setInterleaved(False)\n",
    "    cam.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "\n",
    "    xout = pipeline.createXLinkOut()\n",
    "    xout.setStreamName(\"video\")\n",
    "    cam.preview.link(xout.input)\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# GStreamer로 프레임 전달 함수\n",
    "def push_frame_to_gstreamer(source, frame):\n",
    "    height, width, channels = frame.shape\n",
    "    buf = Gst.Buffer.new_allocate(None, frame.nbytes, None)\n",
    "    buf.fill(0, frame.tobytes())\n",
    "    buf.pts = Gst.util_uint64_scale(Gst.CLOCK_TIME_NONE, 1, Gst.SECOND)\n",
    "\n",
    "    caps = Gst.Caps.from_string(f\"video/x-raw,format=BGR,width={width},height={height},framerate=30/1\")\n",
    "    source.set_property(\"caps\", caps)\n",
    "    source.emit(\"push-buffer\", buf)\n",
    "\n",
    "# GStreamer에서 처리된 메타데이터 확인\n",
    "def process_metadata(sample):\n",
    "    buffer = sample.get_buffer()\n",
    "    print(\"Metadata buffer received:\", buffer)\n",
    "\n",
    "# GStreamer에서 처리된 프레임 가져오기\n",
    "def get_frame_from_gstreamer(sink):\n",
    "    sample = sink.emit(\"pull-sample\")\n",
    "    if not sample:\n",
    "        return None\n",
    "\n",
    "    process_metadata(sample)\n",
    "    return True\n",
    "\n",
    "# 메인 실행 함수\n",
    "def main():\n",
    "    gst_pipeline = None\n",
    "    try:\n",
    "        # GStreamer 파이프라인 생성\n",
    "        print(\"Creating GStreamer pipeline...\")\n",
    "        gst_pipeline = create_gstreamer_pipeline()\n",
    "        source = gst_pipeline.get_by_name(\"source\")\n",
    "        sink = gst_pipeline.get_by_name(\"sink\")\n",
    "        gst_pipeline.set_state(Gst.State.PLAYING)\n",
    "        print(\"GStreamer pipeline set to PLAYING.\")\n",
    "\n",
    "        # OAK-D 파이프라인 생성\n",
    "        print(\"Creating OAK-D pipeline...\")\n",
    "        oak_d_pipeline = create_oak_d_pipeline()\n",
    "        print(\"OAK-D pipeline created.\")\n",
    "\n",
    "        # OAK-D 장치 연결 및 데이터 처리\n",
    "        with dai.Device(oak_d_pipeline) as device:\n",
    "            print(\"OAK-D device initialized.\")\n",
    "            queue = device.getOutputQueue(name=\"video\", maxSize=4, blocking=False)\n",
    "\n",
    "            while True:\n",
    "                print(\"Reading frame from OAK-D...\")\n",
    "                in_frame = queue.get()\n",
    "                frame = in_frame.getCvFrame()\n",
    "\n",
    "                print(\"Frame received. Pushing to GStreamer...\")\n",
    "                push_frame_to_gstreamer(source, frame)\n",
    "\n",
    "                if get_frame_from_gstreamer(sink):\n",
    "                    print(\"Inference completed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if gst_pipeline:\n",
    "            gst_pipeline.set_state(Gst.State.NULL)\n",
    "        print(\"Pipeline stopped.\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
