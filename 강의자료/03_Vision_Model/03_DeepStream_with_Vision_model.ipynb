{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. DeepStream with Vision model\n",
    "\n",
    "이 강의에서는 ONNX 파일을 TensorRT 엔진 파일로 변환하여 DeepStream 애플리케이션에 배포하는 과정을 안내합니다. TensorRT는 딥러닝 모델의 추론을 최적화하여 고성능, 저지연 결과를 제공합니다.\n",
    "\n",
    "## 03-2. DeepStream과 TensorRT 개요\n",
    "- **DeepStream**: NVIDIA의 스트림 기반 비디오 분석 SDK로, TensorRT 엔진 파일을 활용하여 실시간 추론을 지원합니다.\n",
    "- **TensorRT 엔진 파일**: ONNX 모델에서 변환된 최적화된 파일로, DeepStream에서 추론에 사용됩니다.\n",
    "\n",
    "### DeepStream 애플리케이션의 기본 구성:\n",
    "1. 소스 입력 (RTSP, 파일 등)\n",
    "2. 추론 수행 (TensorRT용 모델 활용)\n",
    "    * ONNX 모델을 TensorR용 engine파일로 변환해서 사용\n",
    "3. 결과 시각화 또는 저장\n",
    "\n",
    "## 03-3. TensorRT와 ONNX\n",
    "- **TensorRT**: NVIDIA에서 개발한 고성능 딥러닝 추론 최적화 및 런타임 라이브러리.\n",
    "- **ONNX (Open Neural Network Exchange)**: 다양한 프레임워크 간 상호 운용성을 지원하는 딥러닝 모델 표현 형식.\n",
    "\n",
    "### TensorRT로 변화하는 이유는?\n",
    "- DeepStream에서는 TensorRT 모델만 사용이 가능합니다.\n",
    "- GPU 활용도를 극대화합니다.\n",
    "- 모델 추론 지연 시간을 줄입니다.\n",
    "- 추론을 위한 메모리 사용을 최적화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# 실습\n",
    "\n",
    "## YOLOv11 모델로 engin파일 만들기\n",
    "\n",
    "### 하드웨어 및 소프트웨어 요구 사항\n",
    "- TensorRT를 지원하는 NVIDIA GPU.\n",
    "- TensorRT SDK 설\n",
    "- Python 환경 (본 실습은 Python 3.10)\n",
    "- DeepStream 설치(테스트 용도, 선택 사항)\n",
    "\n",
    "### 라이브러리 및 의존성\n",
    "Python 환경에서 다음 명령어로 필요한 라이브러리를 설치합니다:\n",
    "```bash\n",
    "$ sudo pip3 install onnx onnxruntime tensorrt pycuda\n",
    "```\n",
    "\n",
    "### YOLOv11 onnx 모델 다운로드\n",
    "YOLO 라이브러리 설치 및 numpy 1.23 버전 설치\n",
    "\n",
    "```bash\n",
    "$ sudo pip3 install ultralytics\n",
    "$ sudo pip3 install numpy==1.23\n",
    "```\n",
    "\n",
    "#### 1. Python 스크립트를 이용한 YOLOv8.pt 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded and saved at: yolov8n.pt\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "#YOLOv8n 모델 다운로드\n",
    "model = YOLO('yolov8n')\n",
    "model_path = 'yolov8n.pt'\n",
    "\n",
    "print(f\"Model downloaded and saved at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. CLI를 이용한 YOLOv11.onnx 다운로드\n",
    "```bash\n",
    "$ yolo export model=yolo11n.pt format=onnx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT 빌더 스크립트 준비\n",
    "\n",
    "TensorRT는 ONNX 모델을 엔진 파일로 변환하는 도구를 제공합니다. 아래는 Python 기반의 접근 방식입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엔진 파일이 yolo11n.engine에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "# TensorRT 로거\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def build_engine(onnx_file_path, engine_file_path):\n",
    "    # 빌더, 네트워크, 설정 생성\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser:\n",
    "        \n",
    "        # 빌더 구성\n",
    "        config = builder.create_builder_config()\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 1 << 30)  # 1GB 워크스페이스 메모리 설정\n",
    "\n",
    "        # ONNX 파일 파싱\n",
    "        with open(onnx_file_path, 'rb') as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                print(\"ONNX 파일 파싱에 실패했습니다.\")\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return None\n",
    "\n",
    "        # 네트워크를 직렬화하여 엔진 생성\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        if serialized_engine is None:\n",
    "            print(\"엔진 생성에 실패했습니다.\")\n",
    "            return None\n",
    "\n",
    "        # 엔진 파일 저장\n",
    "        with open(engine_file_path, \"wb\") as f:\n",
    "            f.write(serialized_engine)\n",
    "        print(f\"엔진 파일이 {engine_file_path}에 저장되었습니다.\")\n",
    "\n",
    "# 예제 사용법\n",
    "build_engine(\"yolo11n.onnx\", \"yolo11n.engine\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엔진 파일 테스트\n",
    "\n",
    "`.engine` 파일이 생성되면, 올바르게 동작하는지 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엔진이 성공적으로 로드되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "\n",
    "# TensorRT 엔진 로드\n",
    "def load_engine(engine_file_path):\n",
    "    with open(engine_file_path, \"rb\") as f:\n",
    "        runtime = trt.Runtime(TRT_LOGGER)\n",
    "        return runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "engine = load_engine(\"yolo11n.engine\")\n",
    "print(\"엔진이 성공적으로 로드되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepStream에서 활용\n",
    "1. **엔진 파일 배치**: `.engine` 파일을 원하는 디렉터리로 이동합니다.\n",
    "2. **DeepStream 구성 업데이트**: `config_infer_primary.txt` 파일을 수정하여 새 엔진 파일을 사용하도록 설정합니다.\n",
    "\n",
    "### **구성 파일 예제**\n",
    "아래는 `config_infer_primary_yolo11n.txt`의 예제입니다:\n",
    "\n",
    "```txt\n",
    "[property]\n",
    "gpu-id=0\n",
    "model-engine-file=yolo11n.engine\n",
    "labelfile-path=labels.txt\n",
    "batch-size=1\n",
    "network-mode=0\n",
    "num-detected-classes=80\n",
    "interval=0\n",
    "gie-unique-id=1\n",
    "```\n",
    "\n",
    "아래는 config_infer_primary_yolo11n.txt를 만드는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_infer_primary_yolo11n.txt 파일이 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# config_infer_primary_yolo11n.txt 파일 생성\n",
    "config_content = \"\"\"[property]\n",
    "gpu-id=0\n",
    "model-engine-file=yolo11n.engine\n",
    "labelfile-path=labels.txt\n",
    "batch-size=1\n",
    "network-mode=0\n",
    "num-detected-classes=80\n",
    "interval=0\n",
    "gie-unique-id=1\n",
    "\"\"\"\n",
    "\n",
    "# 파일 쓰기\n",
    "with open(\"config_infer_primary_yolo11n.txt\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"config_infer_primary_yolo11n.txt 파일이 생성되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COCO 데이터셋의 클래스 이름으로 label.txt 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels.txt 파일 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "# label.txt 생성\n",
    "labels = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "    \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
    "    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
    "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
    "    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
    "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
    "    \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"TV\", \"laptop\", \"mouse\",\n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
    "    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "with open(\"labels.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(labels))\n",
    "\n",
    "print(\"labels.txt 파일 생성 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepStream 애플리케이션 코드\n",
    "이제 Python을 사용하여 DeepStream 애플리케이션을 실행하는 코드를 작성합니다. 이 코드는 TensorRT 엔진 파일을 로드하고, 비디오 스트림에서 추론을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import gi\n",
    "import threading\n",
    "\n",
    "# GStreamer와 DeepStream 플러그인을 임포트합니다.\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import Gst\n",
    "\n",
    "# GStreamer 초기화\n",
    "Gst.init(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepStream 파이프라인 생성\n",
    "pipeline = Gst.parse_launch(\n",
    "    \"nvv4l2camerasrc device=/dev/video0 ! video/x-raw(memory:NVMM), width=1280, height=720, framerate=30/1 ! \"\n",
    "    \"nvvidconv ! video/x-raw(memory:NVMM), format=NV12 ! mux.sink_0 \"\n",
    "    \"nvstreammux name=mux batch-size=1 width=1280 height=720 ! \"\n",
    "    \"nvinfer config-file-path=config_infer_primary_yolo11n.txt ! nvdsosd ! \"\n",
    "    \"nveglglessink sync=false\"\n",
    ")\n",
    "\n",
    "# nvstreammux 설정\n",
    "mux = pipeline.get_by_name(\"mux\")\n",
    "if mux:\n",
    "    mux.set_property(\"width\", 1280)\n",
    "    mux.set_property(\"height\", 720)\n",
    "    mux.set_property(\"batch-size\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using winsys: x11 \n",
      "Setting min object dimensions as 16x16 instead of 1x1 to support VIC compute mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0:00:04.025289573 \u001b[31m162288\u001b[00m 0xaaaafb37b610 \u001b[33;01mWARN   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:681:gst_nvinfer_logger:<nvinfer0>\u001b[00m NvDsInferContext[UID 1]: Warning from NvDsInferContextImpl::initialize() <nvdsinfer_context_impl.cpp:1243> [UID = 1]: Warning, OpenCV has been deprecated. Using NMS for clustering instead of cv::groupRectangles with topK = 20 and NMS Threshold = 0.5\n",
      "0:00:04.306949454 \u001b[31m162288\u001b[00m 0xaaaafb37b610 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:684:gst_nvinfer_logger:<nvinfer0>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::deserializeEngineAndBackend() <nvdsinfer_context_impl.cpp:2092> [UID = 1]: deserialized trt engine from :/home/paymentinapp/Desktop/lecture/DeepStream/강의자료/03_Vision_Model/yolo11n.engine\n",
      "0:00:04.307054416 \u001b[31m162288\u001b[00m 0xaaaafb37b610 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer.cpp:684:gst_nvinfer_logger:<nvinfer0>\u001b[00m NvDsInferContext[UID 1]: Info from NvDsInferContextImpl::generateBackendContext() <nvdsinfer_context_impl.cpp:2195> [UID = 1]: Use deserialized engine model: /home/paymentinapp/Desktop/lecture/DeepStream/강의자료/03_Vision_Model/yolo11n.engine\n",
      "0:00:04.333139606 \u001b[31m162288\u001b[00m 0xaaaafb37b610 \u001b[36mINFO   \u001b[00m \u001b[00m             nvinfer gstnvinfer_impl.cpp:343:notifyLoadModelStatus:<nvinfer0>\u001b[00m [UID 1]: Load new model:config_infer_primary_yolo11n.txt sucessfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit layer support has been deprecated\n",
      "INFO: [Implicit Engine Info]: layers num: 0\n",
      "\n",
      "DeepStream 실시간 추론 실행 중...\n",
      "파이프라인 정리 완료\n"
     ]
    }
   ],
   "source": [
    "# 실행 상태 플래그\n",
    "running = True\n",
    "\n",
    "\n",
    "def input_listener():\n",
    "    \"\"\"키보드 입력을 통해 실행 종료.\"\"\"\n",
    "    global running\n",
    "    input(\"실시간 추론을 종료하려면 Enter 키를 누르세요.\\n\")\n",
    "    running = False\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    \"\"\"DeepStream 파이프라인 실행.\"\"\"\n",
    "    global running\n",
    "    try:\n",
    "        # 파이프라인 실행\n",
    "        pipeline.set_state(Gst.State.PLAYING)\n",
    "        print(\"DeepStream 실시간 추론 실행 중...\")\n",
    "\n",
    "        # 키보드 입력 감지 쓰레드 시작\n",
    "        listener_thread = threading.Thread(target=input_listener, daemon=True)\n",
    "        listener_thread.start()\n",
    "\n",
    "        # 메시지 처리\n",
    "        bus = pipeline.get_bus()\n",
    "        while running:\n",
    "            msg = bus.timed_pop_filtered(100 * Gst.MSECOND, Gst.MessageType.ERROR | Gst.MessageType.EOS)\n",
    "            if msg:\n",
    "                msg_type = msg.type\n",
    "                if msg_type == Gst.MessageType.ERROR:\n",
    "                    err, debug = msg.parse_error()\n",
    "                    print(f\"에러: {err}, 디버그 정보: {debug}\")\n",
    "                    break\n",
    "                elif msg_type == Gst.MessageType.EOS:\n",
    "                    print(\"End-Of-Stream 도달\")\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\"파이프라인 실행 중 오류 발생: {e}\")\n",
    "    finally:\n",
    "        # 파이프라인 종료 및 정리\n",
    "        pipeline.set_state(Gst.State.NULL)\n",
    "        pipeline.get_state(Gst.CLOCK_TIME_NONE)\n",
    "        print(\"파이프라인 정리 완료\")\n",
    "\n",
    "\n",
    "# 실행\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['output0']\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "model = onnx.load('yolo11n.onnx')\n",
    "print([node.name for node in model.graph.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
